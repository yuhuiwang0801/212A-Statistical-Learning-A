---
title: "Biostat 212a Homework 4"
subtitle: "Due Mar. 5, 2024 @ 11:59PM"
author: "Yuhui Wang, UID: 606332401"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

```{r setup}
library(ggplot2)
library(tidymodels)
library(tidyverse)
library(ape)
library(ggtree)
library(ISLR2)
library(gtsummary)
library(GGally)
library(ranger)
library(cli)
library(xgboost)
```

## ISL Exercise 8.4.3 (10pts)
> Consider the Gini index, classification error, and cross-entropy in a simple
> classification setting with two classes. Create a single plot that displays
> each of these quantities as a function of $\hat{p}_{m1}$. The $x$-axis should
> display $\hat{p}_{m1}$, ranging from 0 to 1, and the $y$-axis should display
> the value of the Gini index, classification error, and entropy.
>
> _Hint: In a setting with two classes, $\hat{p}_{m1} = 1 - \hat{p}_{m2}$. You
> could make this plot by hand, but it will be much easier to make in `R`._

**Answer:**
Gini index is defined as:
$$G = \sum_{k=1}^{K} \hat{p}_{mk}(1 - \hat{p}_{mk})$$
for binary setting, it is: $G = 2\hat{p}_{m1}(1 - \hat{p}_{m1})$

Cross-entropy is defined as:
$$D = -\sum_{k=1}^{K} \hat{p}_{mk}\log(\hat{p}_{mk})$$
for binary setting, it is: $D = -\hat{p}_{m1}\log(\hat{p}_{m1}) - (1 - \hat{p}_{m1})\log(1 - \hat{p}_{m1})$

Classification error is defined as:
$$E = 1 - \max_k(\hat{p}_{mk})$$
for binary setting, it is: $E = 1 - \max(\hat{p}_{m1}, 1 - \hat{p}_{m1})$

Then, we can plot:
```{r}
p <- seq(0, 1, length.out = 100)

df <- data.frame(
    x = p,
    "Gini index" = p * (1 - p) * 2,
    "Cross-entropy" = -(p * log(p) + (1 - p) * log(1 - p)),
    "Classification error" = 1 - pmax(p, 1 - p),
    check.names = FALSE
  ) 

df |> pivot_longer(!x) |>
  ggplot(aes(x = x, y = value, color = name)) + 
    geom_line(na.rm = TRUE)
```

## ISL Exercise 8.4.4 (10pts)
> This question relates to the plots in Figure 8.14.
>
> a. Sketch the tree corresponding to the partition of the predictor space
>    illustrated in the left-hand panel of Figure 8.14. The numbers inside the
>    boxes indicate the mean of $Y$ within each region.

```{r}
#Add tree content
tree_content <- "(((3:1.5,(10:1,0:1)A:1)B:1,15:2)C:1,5:2)D;"
tree_8.14 <- ape::read.tree(text = tree_content)
tree_8.14$node.label <- c("X1 < 1", "X2 < 1", "X1 < 0", "X2 < 0")

#Plot
ggtree(tree_8.14, ladderize = FALSE) + scale_x_reverse() + coord_flip() +
  geom_tiplab(vjust = 2, hjust = 0.5, size = 2) + 
  geom_text2(aes(label=label, subset=!isTip), hjust = -0.5, vjust = -1, size = 2)
```

> b. Create a diagram similar to the left-hand panel of Figure 8.14, using the
>    tree illustrated in the right-hand panel of the same figure. You should
>    divide up the predictor space into the correct regions, and indicate the
>    mean for each region.

```{r}
lines_data <- data.frame(
  x = c(1, -1, 0, -1),
  xend = c(1, 2, 0, 2),
  y = c(0, 2, 1, 1),
  yend = c(1, 2, 2, 1),
  col = c("blue", "red", "blue", "red"),
  lty = c("dashed", "dashed", "dashed", "dashed")
)

texts_data <- data.frame(
  x = c(0, 1.5, -0.5, 1, 0.5),
  y = c(0.5, 0.5, 1.5, 1.5, 2.5),
  label = c("-1.80", "0.63", "-1.06", "0.21", "2.49")
)

ggplot() +
  geom_segment(data = lines_data, aes(x = x, y = y, xend = xend, yend = yend, colour = col, linetype = lty)) +
  geom_text(data = texts_data, aes(x = x, y = y, label = label)) +
  scale_x_continuous(name = "X1", limits = c(-1, 2), expand = c(0, 0)) +
  scale_y_continuous(name = "X2", limits = c(0, 3), expand = c(0, 0)) +
  theme_minimal() +
  theme(
    panel.grid = element_blank(),
    panel.border = element_rect(colour = "black", fill = NA, size = 0.5),
    axis.line = element_line(color = "black"),
    axis.ticks = element_line(color = "black"),
    axis.text = element_text(color = "black"),
    legend.position = "none"
  ) +
  guides(colour = FALSE, linetype = FALSE) +
  scale_color_identity() +
  scale_linetype_identity()

```

## ISL Exercise 8.4.5 (10pts)
> Suppose we produce ten bootstrapped samples from a data set containing red and
> green classes. We then apply a classification tree to each bootstrapped sample
> and, for a specific value of $X$, produce 10 estimates of
> $P(\textrm{Class is Red}|X)$:
> $$0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, \textrm{and } 0.75.$$
> There are two common ways to combine these results together into a single
> class prediction. One is the majority vote approach discussed in this chapter.
> The second approach is to classify based on the average probability. In this
> example, what is the final classification under each of these two approaches?

```{r}
p <- c(0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, 0.75)
```

For majority vote:
```{r}
# majority vote
print(ifelse(mean(p > 0.5), "red", "green")) 
```

For average probability:
```{r}
# average probability
print(ifelse(mean(p) > 0.5, "red", "green")) 
```


## ISL Lab 8.3. `Boston` data set (30pts)

Follow the machine learning workflow to train regression tree, random forest, and boosting methods for predicting `medv`. Evaluate out-of-sample performance on a test set.


```{r}
Boston <- Boston %>% filter(!is.na(medv))
```

Initial split into test and non-test sets:
```{r}
set.seed(1)
data_split <- initial_split(
  Boston, 
  prop = 0.9
  )
data_split
```
```{r}
Boston_other <- training(data_split)
dim(Boston_other)
```
```{r}
Boston_test <- testing(data_split)
dim(Boston_test)
```

Recipe for R:
```{r}
tree_recipe <- 
  recipe(
    medv ~ ., 
    data = Boston_other
  ) %>%
  step_dummy(all_nominal()) %>%
  step_naomit(medv) %>%
  step_zv(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors())

tree_recipe
```
Model

**Regression tree:**
```{r}
regtree_mod <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = 5,
  mode = "regression",
  engine = "rpart"
  ) 
regtree_mod
```
Workflow
```{r}
tree_wf_rt <- workflow() %>%
  add_recipe(tree_recipe) %>%
  add_model(regtree_mod)
tree_wf_rt
```
Tuning grid
```{r}
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = c(100, 5))
```

Cross-validation
```{r}
# Set cross-validation partitions
set.seed(1)
folds <- vfold_cv(Boston_other, v = 5)
folds
```

```{r}
# Fit cross-validation
tree_fit <- tree_wf_rt %>%
  tune_grid(
    resamples = folds,
    grid = tree_grid,
    metrics = metric_set(rmse, rsq)
    )
tree_fit
```

```{r}
# Visualize CV results
tree_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "rmse") %>%
  mutate(tree_depth = as.factor(tree_depth)) %>%
  ggplot(mapping = aes(x = cost_complexity, y = mean, color = tree_depth)) +
  geom_point() + 
  geom_line() + 
  labs(x = "cost_complexity", y = "CV mse")
```

Finalize the model
```{r}
tree_fit %>%
  show_best("rmse")
```

```{r}
best_tree <- tree_fit %>%
  select_best("rmse")
best_tree
```

```{r}
# Final workflow
final_wf_rt <- tree_wf_rt %>%
  finalize_workflow(best_tree)
final_wf_rt
```
```{r}
# Fit the whole training set, then predict the test cases
final_fit_rt <- 
  final_wf_rt %>%
  last_fit(data_split)
final_fit_rt
```

```{r}
# Test metrics
final_fit_rt %>% 
  collect_metrics()
```

Visualize the model
```{r}
library(rpart.plot)
final_tree <- extract_workflow(final_fit_rt)
final_tree
```

```{r}
final_tree %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE)
```
```{r}
library(vip)

final_tree %>% 
  extract_fit_parsnip() %>% 
  vip()
```
Model
**Random forest:**
```{r}
rf_mod <- 
  rand_forest(
    mode = "regression",
    # Number of predictors randomly sampled in each split
    mtry = tune(),
    # Number of trees in ensemble
    trees = tune()
  ) %>% 
  set_engine("ranger")
rf_mod
```

Workflow
```{r}
rf_wf <- workflow() %>%
  add_recipe(tree_recipe) %>%
  add_model(rf_mod)
rf_wf
```

Tuning grid
```{r}
param_grid <- grid_regular(
  trees(range = c(100L, 300L)), 
  mtry(range = c(1L, 5L)),
  levels = c(3, 5)
  )
param_grid
```

Cross-validation
```{r}
set.seed(1)

folds <- vfold_cv(Boston_other, v = 5)
folds
```

```{r}
# Fit cross-validation
rf_fit <- rf_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(rmse, rsq)
    )
rf_fit
```

Visualize CV results:
```{r}
rf_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "rmse") %>%
  mutate(mtry = as.factor(mtry)) %>%
  ggplot(mapping = aes(x = trees, y = mean, color = mtry)) +
  geom_line() + 
  labs(x = "Num. of Trees", y = "CV mse")
```
```{r}
rf_fit %>%
  show_best("rmse")
```
```{r}
best_rf <- rf_fit %>%
  select_best("rmse")
best_rf
```
Finalize the model
```{r}
# Final workflow
final_wf <- rf_wf %>%
  finalize_workflow(best_rf)
final_wf
```

```{r}
# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit
```

```{r}
# Test metrics
final_fit %>% 
  collect_metrics()
```

**Boosting:**
```{r}
gb_mod <- 
  boost_tree(
    mode = "regression",
    trees = 1000, 
    tree_depth = tune(),
    learn_rate = tune()
  ) %>% 
  set_engine("xgboost")
gb_mod
```

Workflow
```{r}
gb_wf <- workflow() %>%
  add_recipe(tree_recipe) %>%
  add_model(gb_mod)
gb_wf
```
Tuning grid
```{r}
param_grid <- grid_regular(
  tree_depth(range = c(1L, 4L)),
  learn_rate(range = c(-3, -0.5), trans = log10_trans()),
  levels = c(4, 10)
  )
param_grid
```
Cross-validation
```{r}
gb_fit <- gb_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(rmse, rsq)
    )
gb_fit
```
Visualize CV results
```{r}
gb_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "rmse") %>%
  ggplot(mapping = aes(x = learn_rate, y = mean, color = factor(tree_depth))) +
  geom_point() +
  geom_line() +
  labs(x = "Learning Rate", y = "CV AUC") +
  scale_x_log10()
```
Show the top 5 models
```{r}
gb_fit %>%
  show_best("rmse")
```
Select the best model
```{r}
best_gb <- gb_fit %>%
  select_best("rmse")
best_gb
```

Finalize the model
```{r}
# Final workflow
final_wf <- gb_wf %>%
  finalize_workflow(best_gb)
final_wf
```

```{r}
# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit
```

```{r}
# Test metrics
final_fit %>% 
  collect_metrics()
```

Overall, the tree model has RMSE of 4.8922, the random forest model has RMSE of 3.5607, and boosting model has RMSE of 3.1804. Therefore, for `Boston` data set, the boosting model has the best performance.



## ISL Lab 8.3 `Carseats` data set (30pts)

Follow the machine learning workflow to train classification tree, random forest, and boosting methods for classifying `Sales <= 8` versus `Sales > 8`. Evaluate out-of-sample performance on a test set.

```{r}
Carseats <- Carseats %>%
  mutate(Sales = ifelse(Sales <= 8, "Low", "High"))
```
Initial split into test and non-test set
```{r}
set.seed(2)

data_split <- initial_split(
  Carseats, 
  prop = 0.9,
  strata = Sales
  )
data_split
```

```{r}
Carseats_other <- training(data_split)
dim(Carseats_other)
```

```{r}
Carseats_test <- testing(data_split)
dim(Carseats_test)
```

Recipe
```{r}
tree_recipe <- 
  recipe(
    Sales ~ ., 
    data = Carseats_other
  ) %>%
  step_naomit(all_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors()) 
tree_recipe
```

Model 
**Classification tree:**
```{r}
classtree_mod <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = 5,
  mode = "classification",
  engine = "rpart"
  ) 
```

Workflow
```{r}
tree_wf <- workflow() %>%
  add_recipe(tree_recipe) %>%
  add_model(classtree_mod) 
tree_wf
```
Tuning grid
```{r}
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = c(100,5))
tree_grid
```
Cross-validation
```{r}
set.seed(2)
folds <- vfold_cv(Carseats_other, v = 5)
folds
```

```{r}
tree_fit <- tree_wf %>%
  tune_grid(
    resamples = folds,
    grid = tree_grid,
    metrics = metric_set(accuracy, roc_auc)
    )
tree_fit
```

Visualize CV results
```{r}
tree_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  mutate(tree_depth = as.factor(tree_depth)) %>%
  ggplot(mapping = aes(x = cost_complexity, y = mean, color = tree_depth)) +
  geom_point() +
  geom_line() +
  labs(x = "cost_complexity", y = "CV ROC AUC", color = "tree_depth")
```

Finalize the model
```{r}
tree_fit %>%
  show_best("roc_auc")
```

```{r}
best_tree <- tree_fit %>%
  select_best("roc_auc")
best_tree
```

```{r}
# Final workflow
final_wf <- tree_wf %>%
  finalize_workflow(best_tree)
final_wf
```

```{r}
# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit
```

```{r}
# Test metrics
final_fit %>% 
  collect_metrics()
```

Visualize the final model
```{r}
library(rpart.plot)
final_tree <- extract_workflow(final_fit)
final_tree
```
```{r}
final_tree %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE)
```

```{r}
library(vip)

final_tree %>% 
  extract_fit_parsnip() %>% 
  vip()
```

**Random forest:**

```{r}
rf_mod <- 
  rand_forest(
    mode = "classification",
    # Number of predictors randomly sampled in each split
    mtry = tune(),
    # Number of trees in ensemble
    trees = tune()
  ) %>% 
  set_engine("ranger")
rf_mod
```

Workflow
```{r}
rf_wf <- workflow() %>%
  add_recipe(tree_recipe) %>%
  add_model(rf_mod)
rf_wf
```

Tuning grid
```{r}
param_grid <- grid_regular(
  trees(range = c(100L, 300L)), 
  mtry(range = c(1L, 5L)),
  levels = c(3, 5)
  )
param_grid
```

Cross-validation
```{r}
set.seed(2)

folds <- vfold_cv(Carseats_other, v = 5)
folds
```

```{r}
rf_fit <- rf_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy)
    )
rf_fit
```
Visualize CV results
```{r}
rf_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  mutate(mtry = as.factor(mtry)) %>%
  ggplot(mapping = aes(x = trees, y = mean, color = mtry)) +
  # geom_point() + 
  geom_line() + 
  labs(x = "Num. of Trees", y = "CV AUC")
```

Show the top 5 models
```{r}
rf_fit %>%
  show_best("roc_auc")
```
Select the best model
```{r}
best_rf <- rf_fit %>%
  select_best("roc_auc")
best_rf
```
Finalize the model
```{r}
# Final workflow
final_wf <- rf_wf %>%
  finalize_workflow(best_rf)
final_wf
```
```{r}
# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit
```
```{r}
# Test metrics
final_fit %>% 
  collect_metrics()
```

Model
**Boosting:**

```{r}
gb_mod <- 
  boost_tree(
    mode = "classification",
    trees = 1000, 
    tree_depth = tune(),
    learn_rate = tune()
  ) %>% 
  set_engine("xgboost")
gb_mod
```
Workflow
```{r}
gb_wf <- workflow() %>%
  add_recipe(tree_recipe) %>%
  add_model(gb_mod)
gb_wf
```

Tuning grid
```{r}
param_grid <- grid_regular(
  tree_depth(range = c(1L, 3L)),
  learn_rate(range = c(-5, 2), trans = log10_trans()),
  levels = c(3, 10)
  )
param_grid
```
Cross-validation
```{r}
gb_fit <- gb_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy)
    )
gb_fit
```

Visualize CV results
```{r}
gb_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  mutate(tree_depth = as.factor(tree_depth)) %>%
  ggplot(mapping = aes(x = learn_rate, y = mean, color = tree_depth)) +
  geom_point() +
  geom_line() +
  labs(x = "Learning Rate", y = "CV AUC") +
  scale_x_log10()
```

Show the top 5 models
```{r}
gb_fit %>%
  show_best("roc_auc")
```
Select the best model
```{r}
best_gb <- gb_fit %>%
  select_best("roc_auc")
best_gb
```

Finalize the model
```{r}
# Final workflow
final_wf <- gb_wf %>%
  finalize_workflow(best_gb)
final_wf
```

```{r}
# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit
```

```{r}
# Test metrics
final_fit %>% 
  collect_metrics()
```

Overall, the classification tree model has accuracy of 0.8049, the random forest model has accuracy of 0.8536585, and the boosting model has accuracy of 0.8536585, which is the same. However, for aoc_auc, random forest is 0.8970588, and boosting is 0.9583333. Therefore, the random forest model is the best performance model for this dataset. 


